{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSV_to_S3_RDS1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNPi3dqnHv6jDBbt1bfLJBi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steveding1/aws-dataengineering/blob/main/CSV_to_S3_RDS1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Get urls from 'http://insideairbnb.com/get-the-data/' compare the date in the url.\n",
        "2.   loop:\n",
        "\n",
        "*   download\n",
        "*   csv.gz to S3\n",
        "*   csv.gz to db(optional)\n",
        "*   successful info to info and table 'log' (optional)\n",
        "*   delete(optional)\n",
        "\n",
        "3.  csv.gz files process and load to db from S3.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BRNByJ8SqntI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKVrfhZXGBBN",
        "outputId": "69c3593e-9726-478c-ec4d-c33a6bef889b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.22.9-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting botocore<1.26.0,>=1.25.9\n",
            "  Downloading botocore-1.25.9-py3-none-any.whl (8.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 9.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.26.0,>=1.25.9->boto3) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 39.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.26.0,>=1.25.9->boto3) (1.15.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.9 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.22.9 botocore-1.25.9 jmespath-1.0.0 s3transfer-0.5.2 urllib3-1.26.9\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Installing collected packages: urllib3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.9\n",
            "    Uninstalling urllib3-1.26.9:\n",
            "      Successfully uninstalled urllib3-1.26.9\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "!pip install boto3\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import boto3\n",
        "from botocore.exceptions import ClientError\n",
        "import os\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import configparser\n",
        "import urllib.request\n",
        "from dateutil.parser import parse\n",
        "import psycopg2\n",
        "import gzip\n",
        "import datetime"
      ],
      "metadata": {
        "id": "60HKmoX4H80U"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_urls(url,ext)->list:\n",
        "  \"\"\"\n",
        "  get all the links end with the {ext}etion from the {url} webpage\n",
        "  \"\"\"\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "  return [a['href']for a in soup.findAll('a', href=re.compile(ext+\"$\"))]\n"
      ],
      "metadata": {
        "id": "ErmWj-IohP3J"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_date(ds:str)->bool:\n",
        "  \"\"\"\n",
        "  test if a string is a legal date format in yyyy-mm-dd\n",
        "  \"\"\"\n",
        "  if re.match(r'\\d{4}-\\d{2}-\\d{2}', ds):\n",
        "    try:\n",
        "        return bool(parse(ds))\n",
        "    except:\n",
        "      False\n",
        "  return False\n",
        "#is_date('2001-13-30')"
      ],
      "metadata": {
        "id": "WpeC1Shbxpqf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parseurl(url:str)->list:\n",
        "  \"\"\"\n",
        "  get nation city and date from a url string \n",
        "  \"\"\"\n",
        "  splits = url.split('/')\n",
        "  try:\n",
        "    nation = splits[splits.index('data.insideairbnb.com')+1]\n",
        "  except ValueError:\n",
        "    print('The url not matches the rule',url)\n",
        "\n",
        "  for i,item in enumerate(splits):\n",
        "    if is_date(item):\n",
        "      date = item\n",
        "      city = splits[i-1]\n",
        "      return [nation, city, date]\n",
        "      break\n",
        "  return False\n",
        "\n",
        "#parseurl(urls[1])"
      ],
      "metadata": {
        "id": "IqnvmVmnJ3IM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def url_tocsv(urls):\n",
        "  \"\"\"\n",
        "  urls list transform and write to a dataframe\n",
        "  \"\"\"\n",
        "  df = pd.DataFrame(columns=[\"nation\", \"city\", \"date\", \"url\"])\n",
        "  for url in urls:\n",
        "    url_df=pd.DataFrame([parseurl(url)+[url]])\n",
        "    url_df.columns=([\"nation\", \"city\", \"date\", \"url\"])\n",
        "    df = pd.concat([df,url_df])\n",
        "  df.to_csv(\"urls.csv\",index=False)\n",
        "#url_tocsv(urls)"
      ],
      "metadata": {
        "id": "WiqCE9NvH8oa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_s3(file_name, bucket, pkey, skey, object_name=None):\n",
        "    \"\"\"Upload a file to an S3 bucket\n",
        "    :param file_name: File to upload\n",
        "    :param bucket: Bucket to upload to\n",
        "    :param object_name: S3 object name. If not specified then file_name is used\n",
        "    :return: True if file was uploaded, else False\n",
        "    \"\"\"\n",
        "    session = boto3.Session(\n",
        "    aws_access_key_id=pkey,\n",
        "    aws_secret_access_key=skey\n",
        ")\n",
        "    # If S3 object_name was not specified, use file_name\n",
        "    if object_name is None:\n",
        "      object_name = os.path.basename(file_name)\n",
        "    s3_client = boto3.client(\n",
        "        's3',\n",
        "         aws_access_key_id=pkey,\n",
        "    aws_secret_access_key=skey)\n",
        "    try:\n",
        "      response = s3_client.upload_file(file_name,bucket,object_name)\n",
        "    except ClientError as e:\n",
        "      logging.error(e)\n",
        "      return False\n",
        "    return True\n",
        "    # Upload the file\n",
        "   "
      ],
      "metadata": {
        "id": "CBmQbhoGI220"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gz_to_db(fullfilename,conf) -> bool:\n",
        "  '''\n",
        "  load a csv gz file to postgres db\n",
        "  '''\n",
        "  parser = configparser.ConfigParser()\n",
        "  parser.read(conf)\n",
        "  sql_host = parser.get(\"postgres\",\"host\")\n",
        "  sql_user = parser.get(\"postgres\",\"username\")\n",
        "  sql_password = parser.get(\"postgres\",\"password\")\n",
        "  sql_db = parser.get(\"postgres\",\"database\")\n",
        "  sql_table = parser.get(\"postgres\",\"table\")\n",
        "  load_query = \"\"\"\n",
        "  COPY {sql_table} FROM STDIN WITH CSV HEADER DELIMITER AS ',' QUOTE AS '\\\"' ;\n",
        "  \"\"\".format(\n",
        "      sql_table=sql_table\n",
        "  )\n",
        "  # Create database connection\n",
        "  sql_connection = psycopg2.connect(\n",
        "      host=sql_host,\n",
        "      user=sql_user,\n",
        "      password=sql_password,\n",
        "      database=sql_db\n",
        "  )\n",
        "  sql_cursor = sql_connection.cursor()\n",
        "  try:\n",
        "    with gzip.open(filename=fullfilename, mode='rb') as local_file:\n",
        "      sql_cursor.copy_expert(sql=load_query, file=local_file)\n",
        "      sql_connection.commit()\n",
        "      return True\n",
        "  except Exception as load_exception:\n",
        "      sql_connection.rollback()\n",
        "      return False\n",
        "      raise Exception(\"SQL LOAD error: \" + str(load_exception)) from load_exception\n",
        "  # close sql cursor and connection\n",
        "  sql_cursor.close()\n",
        "  sql_connection.close()"
      ],
      "metadata": {
        "id": "ZldIjMSKJWac"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def down_upload(conf,keepfiles=False,down=True,to_db=False,to_s3=True):\n",
        "  parser = configparser.ConfigParser()\n",
        "  parser.read(conf)\n",
        "  access_key = parser.get(\"boto_config\",\"access_key\")\n",
        "  secret_key = parser.get(\"boto_config\",\"secret_key\")\n",
        "  bucket_name = parser.get(\"boto_config\",\"bucket_name\")\n",
        "  extention = parser.get(\"website\",\"extention\")\n",
        "  \n",
        "  downloads = pd.read_csv('urls.csv')\n",
        "  if not os.path.exists('download'):\n",
        "    os.makedirs('download')\n",
        "  for _,download in downloads.iterrows():\n",
        "    url = download['url']\n",
        "    fname = download['nation']+'_'+download['city']+extention\n",
        "    print('processing: ' + fname)\n",
        "    fullfilename = os.path.join('download', fname)\n",
        "    if down:\n",
        "      urllib.request.urlretrieve(url, fullfilename)\n",
        "    if to_s3:\n",
        "      if upload_s3(fullfilename, bucket_name, access_key, secret_key):\n",
        "        with open('success.log','a') as f:\n",
        "          f.write(url+','+datetime.datetime.now().strftime('%y-%m-%d,%H%M%S')+',s3\\n')\n",
        "    if to_db:\n",
        "      if gz_to_db(fullfilename,conf):\n",
        "        with open('success.log','a') as f:\n",
        "          f.write(url+','+datetime.datetime.now().strftime('%y-%m-%d,%H%M%S')+',db\\n')\n",
        "    if keepfiles is False:\n",
        "      os.remove(fullfilename)"
      ],
      "metadata": {
        "id": "7nRWd3knNBqy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  CONF = \"conf.conf\"\n",
        "  parser = configparser.ConfigParser()\n",
        "  parser.read(CONF)\n",
        "  website = parser.get(\"website\",\"url\")\n",
        "  extention = parser.get(\"website\",\"extention\")\n",
        "  urls = get_urls(website,extention)\n",
        "  url_tocsv(urls)\n",
        "  down_upload(conf=CONF,keepfiles=True,to_db=True,to_s3=False,down=False)"
      ],
      "metadata": {
        "id": "2nR7YR7siPXl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54UCTvjCXS-l",
        "outputId": "dfbd425f-1461-435e-8048-9592ad92a9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing: the-netherlands_amsterdam.listings.csv.gz\n",
            "processing: belgium_antwerp.listings.csv.gz\n",
            "processing: united-states_asheville.listings.csv.gz\n",
            "processing: greece_athens.listings.csv.gz\n",
            "processing: united-states_austin.listings.csv.gz\n",
            "processing: thailand_bangkok.listings.csv.gz\n",
            "processing: spain_barcelona.listings.csv.gz\n",
            "processing: australia_barossa-valley.listings.csv.gz\n",
            "processing: australia_barwon-south-west-vic.listings.csv.gz\n",
            "processing: china_beijing.listings.csv.gz\n",
            "processing: belize_belize.listings.csv.gz\n",
            "processing: italy_bergamo.listings.csv.gz\n",
            "processing: germany_berlin.listings.csv.gz\n",
            "processing: italy_bologna.listings.csv.gz\n",
            "processing: france_bordeaux.listings.csv.gz\n",
            "processing: united-states_boston.listings.csv.gz\n",
            "processing: united-kingdom_bristol.listings.csv.gz\n",
            "processing: united-states_broward-county.listings.csv.gz\n",
            "processing: belgium_brussels.listings.csv.gz\n",
            "processing: argentina_buenos-aires.listings.csv.gz\n",
            "processing: united-states_cambridge.listings.csv.gz\n",
            "processing: south-africa_cape-town.listings.csv.gz\n",
            "processing: united-states_chicago.listings.csv.gz\n",
            "processing: united-states_clark-county-nv.listings.csv.gz\n",
            "processing: united-states_columbus.listings.csv.gz\n",
            "processing: denmark_copenhagen.listings.csv.gz\n",
            "processing: greece_crete.listings.csv.gz\n",
            "processing: united-states_dallas.listings.csv.gz\n",
            "processing: united-states_denver.listings.csv.gz\n",
            "processing: ireland_dublin.listings.csv.gz\n",
            "processing: united-kingdom_edinburgh.listings.csv.gz\n",
            "processing: spain_euskadi.listings.csv.gz\n",
            "processing: italy_florence.listings.csv.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = configparser.ConfigParser()\n",
        "parser.read('conf.conf')\n",
        "sql_host = parser.get(\"postgres\",\"host\")\n",
        "sql_user = parser.get(\"postgres\",\"username\")\n",
        "sql_password = parser.get(\"postgres\",\"password\")\n",
        "sql_db = parser.get(\"postgres\",\"database\")\n",
        "sql_table = parser.get(\"postgres\",\"table\")\n",
        "\n",
        "# Create database connection\n",
        "sql_connection = psycopg2.connect(\n",
        "    host=sql_host,\n",
        "    user=sql_user,\n",
        "    password=sql_password,\n",
        "    database=sql_db\n",
        ")\n",
        "sql_cursor = sql_connection.cursor()\n",
        "sql_cursor.execute('select * from '+sql_table)\n",
        "sql_cursor.fetchall()"
      ],
      "metadata": {
        "id": "z-FFWamoPvNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9650a13f-0a3d-412e-dbbc-1384408eec5e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}